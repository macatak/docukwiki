TryHackme Google Dorking [[https://www.tryhackme.com/room/googledorking]]

**Search Engines 101**

These essentials in surfing the internet use “Crawlers” or “Spiders” to search for this content across the World Wide Web

Once a web crawler discovers a domain such as mywebsite.com, it will index the entire contents of the domain, looking for keywords and other miscellaneous information

Crawlers attempt to traverse, termed as crawling, every URL and file that they can find. If “mywebsite.com” had the keywords “Apple", “Banana” and “Pear” but also had a URL to another website “anotherwebsite.com”, the crawler will then attempt to traverse everything on that URL (anotherwebsite.com) and retrieve the contents of everything within that domain respectively

The crawler initially finds “mywebsite.com”, where it crawls the contents of the website - finding the same keywords (“Apple", “Banana” and “Pear”) as before, but it has additionally found an external URL. Once the crawler is complete on “mywebsite.com”, it'll proceed to crawl the contents of the website “anotherwebsite.com”, where the keywords ("Tomatoes", “Strawberries” and “Pineapples”) are found on it. The crawler's dictionary now contains the contents of both “mywebsite.com” and “anotherwebsite.com”, which is then stored and saved within the search engine.


**How does the "Search Engine" decide on the hierarchy of the domains that are displayed to the user?**

A logical presumption would be that website 1 -> 3 would be displayed...But that's not how real-world domains work and/or are named.

Answers
  - index
  - crawling (not crawl)
  - keywords


**Search Engine Optimisation**

Search Engine Optimisation or SEO is a prevalent and lucrative topic in modern-day search engines. In fact, so much so, that entire businesses capitalise on improving a domains SEO “ranking”. At an abstract view, search engines will “prioritise” those domains that are easier to index. There are many factors in how “optimal” a domain is - resulting in something similar to a point-scoring system.

To highlight a few influences on how these points are scored, factors such as:

• How responsive your website is to the different browser types I.e. Google Chrome, Firefox and Internet Explorer - this includes Mobile phones!

• How easy it is to crawl your website, or if crawling is even allowed, through the use of "Sitemaps"

• What kind of keywords your website has 

There is a lot of complexity in how the various search engines individually "point-score" or rank these domains - including vast algorithms. Naturally, the companies running these search engines such as Google don't share exactly how the hierarchic view of domains ultimately ends up. Although, as these are businesses at the end of the day, you can pay to advertise/boost the order of which your domain is displayed.

There are various online tools [[https://seositecheckup.com/]]

**Who or What Regulates these "Crawlers"?**

Aside from the search engines who provide these "Crawlers", website/web-server owners themselves ultimately stipulate what content "Crawlers" can scrape. Search engines will want to retrieve everything from a website - but there are a few cases where we wouldn't want all of the contents of our website to be indexed

Questions
  - 1-2 - https://seositecheckup.com/seo-audit/tryhackme.com
  - 3-4 - https://app.neilpatel.com/en/seo_analyzer/site_audit?domain=http%3A%2F%2Fgoogledorking.cmnatic.co.uk%2F&submit=Analyze+Website

Answers
  - 83/100
  - googledorking.cmnatic.co.uk


**robots.txt**

This file is the first thing indexed by "Crawlers" when visiting a website.
This file must be served at the root directory
Defines the permissions the crawler has to the website.
Defineswhat type of crawler is allowed (I.e. You only want Google's crawler to index your site and not MSN's).
Can specify what files and directories that we do or don't want to be indexed by the crawler.


Keyword		Function
User-agent	Specify the type of crawler that can index your site (the asterisk being a wildcard, allowing all "User-agents"
Allow		Specify the directories or file(s) that the crawler can index
Disallow	Specify the directories or file(s) that the crawler cannot index
Sitemap		Provide a reference to where the sitemap is located


allow all crawlers and sitemap location:
<nowiki>
User-agent: *
Allow: /
Sitemap: http://snafu.com.sitemap.xml
</nowiki>


  * Any crawler can index the site
  *  The crawler can index every other content that isn't contained within "/super-secret-directory/".
  * The crawler index all the contents within "/not-a-secret/", but will not index anything contained within the sub-directory "/but-this-is/".
<nowiki>
User-agent: *
Disallow: /super-secret-directory/
Disallow: /not-a-secret/but-this-is/
Sitemap:  http://snafu.com.sitemap.xml
</nowiki>


  * The crawler "Googlebot" is allowed to index the entire site ("Allow: /")
  * The crawler "msnbot" is not allowed to index the site (Disallow: /")
<nowiki>
User-agent: Googlebot
Allow: /
User-agent: msnbot
Disallow: /
</nowiki>

Regex is supported
  * Any crawler can index the site
  * The crawler cannot index any file that has the extension of .ini within any directory/sub-directory using ("$") of the site.
<nowiki>
User-agent: *
Disallow: /*.ini$
Sitemap:  http://snafu.com.sitemap.xml
</nowiki>

Answers
  - ablog.com/robots.txt
  - sitemap.xml
  - User-Agent: Bingbot
4 - .conf


**Sitemaps**

Comparable to geographical maps 
helpful for crawlers, as they specify the necessary routes to find content on the domain.
Sitemaps” are XML formatted
The presence of "Sitemaps" holds a fair amount of weight in influencing the "optimisation" and favorability of a website.

Why are "Sitemaps" so Favourable for Search Engines?
The easier a website is to "Crawl", the more optimised it is for the "Search Engine"
Resources like "Sitemaps" are extremely helpful for "Crawlers" as the necessary routes to content are already provided! All the crawler has to do is scrape this content - rather than going through the process of manually finding and scraping.

1 - xml
2 - map
3 - route


**What is Google Dorking?**

Using Google for Advanced Searching

Google's Advanced Operators [[https://www.google.com/advanced_search]]
DuckDuckGo Search Syntax [[https://help.duckduckgo.com/duckduckgo-help-pages/results/syntax/]]]]

narrow down our search query, we can use quotation marks. Google will interpret everything in between these quotation marks as exact and only return the results of the exact phrase provided


We can use terms such as “site” (bbc.co.uk) and a query ("gchq news") to search the specified site for the keyword we have provided to filter out content that may be harder to find otherwise
	site:bbc.co.uk gchq news
	
A few common terms we can search and combine include:
filetype: 	Search for a file by its extension (e.g. PDF)
cache:		View Google's Cached version of a specified URL
intitle:	The specified phrase MUST appear in the title of the page

search for all PDF's on bbc.co.uk
	site:bbc.co.uk filetype:pdf

Example searches
intitle:index.of

Answers
  -  site:bbc.co.uk flood defences
  - filetype:
  -  intitle: .login

Great list of useful InfoSec searches [[https://www.exploit-db.com/google-hacking-database]]